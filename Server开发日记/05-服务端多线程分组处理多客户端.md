# 服务端多线程分组处理多客户端

## 1. 单线程服务端弊端

客户端高频请求时，单线程服务端处理连接请求过慢

### 1.1 单线程模式下性能瓶颈测试

根据性能探查器报告，单线程模式下服务端资源大多消耗在`select`模型上

- 将客户端`socket`加入`fdRead`
- 在`fdRead`中轮询是否有数据可读

**代码层面上优化**：

- 当前处理逻辑是不论客户端是否改变，都将所有客户端重新加入`fdRead`数组
- 因此，我们可以定义一个备份数组，当客户端无改变时，令新`fdRead`数组直接拷贝备份数组
- 当客户端发生改变时，再将所有客户端重新加入

**多线程优化：**

- 利用多线程平摊消耗

## 2. 服务端多线程模型

采用生产者-消费者设计模式

![image-20211012212048439](C:\Users\Sakura\AppData\Roaming\Typora\typora-user-images\image-20211012212048439.png)

生产者线程用于接收客户端连接，当接收了一个新连接，会产生一个新的socket，将此socket传递给另外一个线程，另外一个线程用来处理socket，即消费者线程。

- 生产者消费者模式就是通过一个容器来解决生产者和消费者的强耦合问题

- 生产者和消费者彼此之间不直接通讯，而通过阻塞队列来进行通讯，所以生产者生产完数据之后不用等待消费者处理，直接扔给阻塞队列，消费者不找生产者要数据，而是直接从阻塞队列里取

- 队列就相当于一个缓冲区，平衡了生产者和消费者的处理能力，这个阻塞队列就是用来给生产者和消费者解耦的

**生产者消费者模型优点：**

1. 解耦（阻塞队列的作用）
2. 支持并发（解耦后，消费和生产可以各自运行）
3. 支持忙闲不均

###  2.1 待完善结构：

​	目前缓冲区只用于传递socket，当消费者线程中recv()返回-1即有客户端断开连接时，消费者线程还需要通知生产者线程，还缺少这样一个反向的逻辑。 

## 3. 代码修改

### 3.1 分离消息处理逻辑(定义网络消息接受处理服务类)

从`EasyTcpServer`中分离处理消息逻辑，分离完成后，`EasyTcpServer`中只剩下`SOCKET`建立、绑定、监听以及接收新客户端连接的逻辑

定义新类型`CELLServer`，用于消息处理

- `CELLServer`中有一个缓冲队列与一个正式队列
- 主线程每次接收到新客户端连接时
  - 遍历所有消息处理对象`CELLServer`，找出缓冲队列中元素最少的一个
  - 将新客户端加入缓冲队列元素最少的消息处理对象`CELLServer`中
  - `CELLServer`运行时，首先判断缓冲队列是否有数据，若有则取出数据，加入到正式客户端队列中
    - 注意，该过程需要加锁
  - 利用`select`模型处理正式客户队列中的数据

```c++
//网络消息接收处理服务类
class CellServer
{
public:
	CellServer(SOCKET sock = INVALID_SOCKET)
	{
		_sock = sock;
		_pNetEvent = nullptr;
	}

	~CellServer()
	{
		Close();
		_sock = INVALID_SOCKET;
	}

	void setEventObj(INetEvent* event)
	{
		_pNetEvent = event;
	}

	//关闭Socket
	void Close()
	{
		if (_sock != INVALID_SOCKET)
		{
#ifdef _WIN32
			for (auto iter : _clients)
			{
				closesocket(iter.second->sockfd());
				delete iter.second;
			}
			//关闭套节字closesocket
			closesocket(_sock);
#else
			for (auto iter : _clients)
			{
				close(iter.second->sockfd());
				delete iter.second;
			}
			//关闭套节字closesocket
			close(_sock);
#endif
			_clients.clear();
		}
	}

	//是否工作中
	bool isRun()
	{
		return _sock != INVALID_SOCKET;
	}

	//处理网络消息
	//备份客户socket fd_set
	fd_set _fdRead_bak;
	//客户列表是否有变化
	bool _clients_change;
	SOCKET _maxSock;
	void OnRun()
	{
		_clients_change = true;
		while (isRun())
		{
			if (!_clientsBuff.empty())
			{//从缓冲队列里取出客户数据
				std::lock_guard<std::mutex> lock(_mutex);
				for (auto pClient : _clientsBuff)
				{
					_clients[pClient->sockfd()] = pClient;
				}
				_clientsBuff.clear();
				_clients_change = true;
			}

			//如果没有需要处理的客户端，就跳过
			if (_clients.empty())
			{
				std::chrono::milliseconds t(1);
				std::this_thread::sleep_for(t);
				continue;
			}

			//伯克利套接字 BSD socket
			fd_set fdRead;//描述符（socket） 集合
			//清理集合
			FD_ZERO(&fdRead);
			if (_clients_change)
			{
				_clients_change = false;
				//将描述符（socket）加入集合
				_maxSock = _clients.begin()->second->sockfd();
				for (auto iter : _clients)
				{
					FD_SET(iter.second->sockfd(), &fdRead);
					if (_maxSock < iter.second->sockfd())
					{
						_maxSock = iter.second->sockfd();
					}
				}
				memcpy(&_fdRead_bak, &fdRead, sizeof(fd_set));
			}
			else {
				memcpy(&fdRead, &_fdRead_bak, sizeof(fd_set));
			}

			///nfds 是一个整数值 是指fd_set集合中所有描述符(socket)的范围，而不是数量
			///既是所有文件描述符最大值+1 在Windows中这个参数可以写0
			int ret = select(_maxSock + 1, &fdRead, nullptr, nullptr, nullptr);
			if (ret < 0)
			{
				printf("select任务结束。\n");
				Close();
				return;
			}
			else if (ret == 0)
			{
				continue;
			}
			
#ifdef _WIN32
			for (int n = 0; n < fdRead.fd_count; n++)
			{
				auto iter  = _clients.find(fdRead.fd_array[n]);
				if (iter != _clients.end())
				{
					if (-1 == RecvData(iter->second))
					{
						if (_pNetEvent)
							_pNetEvent->OnNetLeave(iter->second);
						_clients_change = true;
						_clients.erase(iter->first);
					}
				}else {
					printf("error. if (iter != _clients.end())\n");
				}

			}
#else
			std::vector<ClientSocket*> temp;
			for (auto iter : _clients)
			{
				if (FD_ISSET(iter.second->sockfd(), &fdRead))
				{
					if (-1 == RecvData(iter.second))
					{
						if (_pNetEvent)
							_pNetEvent->OnNetLeave(iter.second);
						_clients_change = false;
						temp.push_back(iter.second);
					}
				}
			}
			for (auto pClient : temp)
			{
				_clients.erase(pClient->sockfd());
				delete pClient;
			}
#endif
		}
	}
	//接收数据 处理粘包 拆分包
	int RecvData(ClientSocket* pClient)
	{

		//接收客户端数据
		char* szRecv = pClient->msgBuf() + pClient->getLastPos();
		int nLen = (int)recv(pClient->sockfd(), szRecv, (RECV_BUFF_SZIE)- pClient->getLastPos(), 0);
		_pNetEvent->OnNetRecv(pClient);
		//printf("nLen=%d\n", nLen);
		if (nLen <= 0)
		{
			//printf("客户端<Socket=%d>已退出，任务结束。\n", pClient->sockfd());
			return -1;
		}
		//将收取到的数据拷贝到消息缓冲区
		//memcpy(pClient->msgBuf() + pClient->getLastPos(), _szRecv, nLen);
		//消息缓冲区的数据尾部位置后移
		pClient->setLastPos(pClient->getLastPos() + nLen);

		//判断消息缓冲区的数据长度大于消息头DataHeader长度
		while (pClient->getLastPos() >= sizeof(DataHeader))
		{
			//这时就可以知道当前消息的长度
			DataHeader* header = (DataHeader*)pClient->msgBuf();
			//判断消息缓冲区的数据长度大于消息长度
			if (pClient->getLastPos() >= header->dataLength)
			{
				//消息缓冲区剩余未处理数据的长度
				int nSize = pClient->getLastPos() - header->dataLength;
				//处理网络消息
				OnNetMsg(pClient, header);
				//将消息缓冲区剩余未处理数据前移
				memcpy(pClient->msgBuf(), pClient->msgBuf() + header->dataLength, nSize);
				//消息缓冲区的数据尾部位置前移
				pClient->setLastPos(nSize);
			}
			else {
				//消息缓冲区剩余数据不够一条完整消息
				break;
			}
		}
		return 0;
	}

	//响应网络消息
	virtual void OnNetMsg(ClientSocket* pClient, DataHeader* header)
	{
		_pNetEvent->OnNetMsg(this, pClient, header);
	}

	void addClient(ClientSocket* pClient)
	{
		std::lock_guard<std::mutex> lock(_mutex);
		//_mutex.lock();
		_clientsBuff.push_back(pClient);
		//_mutex.unlock();
	}

	void Start()
	{
        //std::mem_fn(),把成员函数转为函数对象，使用对象指针或对象（引用）进行绑定
		_thread = std::thread(std::mem_fn(&CellServer::OnRun), this);
		_taskServer.Start();
	}

	size_t getClientCount()
	{
		return _clients.size() + _clientsBuff.size();
	}

	void addSendTask(ClientSocket* pClient, DataHeader* header)
	{
		CellSendMsg2ClientTask* task = new CellSendMsg2ClientTask(pClient, header);
		_taskServer.addTask(task);
	}
private:
	SOCKET _sock;
	//正式客户队列
	std::map<SOCKET,ClientSocket*> _clients; 
	//缓冲客户队列
	std::vector<ClientSocket*> _clientsBuff;
	//缓冲队列的锁
	std::mutex _mutex;
    //为了方便管理，定义一个线程成员
	std::thread _thread;
	//网络事件对象
	INetEvent* _pNetEvent;
	//
	CellTaskServer _taskServer;
};
```

### 3.2 将消息处理类型用于EasyTcpServer

- 在`EasyTcpServer`成员变量中添加一个消息处理对象的数组

  ```c++
  vector<CellServer*> _cellServers
  ```

- 定义消息处理线程启动方法

  ```c++
  void Start(int nCellServer)
  	{
  		for (int n = 0; n < nCellServer; n++)
  		{
  			auto ser = new CellServer(_sock);
  			_cellServers.push_back(ser);
  			//注册网络事件接受对象
  			ser->setEventObj(this);
  			//启动消息处理线程
  			ser->Start();
  		}
  	}
  ```

- 将新客户端分配给客户数量最少的消息线程

  ```c++
  void addClientToCellServer(ClientSocket* pClient)
  	{
  		//查找客户数量最少的CellServer消息处理对象
  		auto pMinServer = _cellServers[0];
  		for(auto pCellServer : _cellServers)
  		{
  			if (pMinServer->getClientCount() > pCellServer->getClientCount())
  			{
  				pMinServer = pCellServer;
  			}
  		}
  		pMinServer->addClient(pClient);
  		OnNetJoin(pClient);
  	}
  ```

  

- 事件通知，告知主线程有客户端退出

  实现一个网络事件接口（纯虚函数），让EasyTcpServer继承此接口，然后在EasyTcpServer中进行具体实现的定义

  ```c++
  //网络事件接口
  class INetEvent
  {
  public:
  	//纯虚函数
  	//客户端加入事件
  	virtual void OnNetJoin(ClientSocket* pClient) = 0;
  	//客户端离开事件
  	virtual void OnNetLeave(ClientSocket* pClient) = 0;
  	//客户端消息事件
  	virtual void OnNetMsg(CellServer* pCellServer, ClientSocket* pClient, DataHeader* header) = 0;
  	//recv事件
  	virtual void OnNetRecv(ClientSocket* pClient) = 0;
  private:
  };
  ```

  

### 3.3 达成目标，迅速稳定连接`10K`客户端



